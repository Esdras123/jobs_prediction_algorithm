{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data and Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import modules\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import  Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nprint(\"TensorFlow Version:\",tf.__version__)\nprint(\"Hub version: \",hub.__version__)\npd.set_option('display.max_colwidth',1000)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading data"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/defi-ia-insa-toulouse\"\nOUTPUT_PATH = \"/kaggle/working\"\ntrain_df = pd.read_json(DATA_PATH+\"/train.json\")\ntest_df = pd.read_json(DATA_PATH+\"/test.json\")\ntrain_label = pd.read_csv(DATA_PATH+\"/train_label.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyse data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.description.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify missing values\ntrain_df.apply(lambda x: sum(x.isnull()), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the target class balance\ntrain_label[\"Category\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#put all the text in lower case\ntrain_df[\"description_lower\"] = [x.lower() for x in train_df.description]\ntest_df[\"description_lower\"] = [x.lower() for x in test_df.description]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport re,string,unicodedata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tokenization of text\ntokenizer=ToktokTokenizer()\n#Setting English stopwords\n#stopword_list=nltk.corpus.stopwords.words('english')\n\nstopword_list = ['in', 'of', 'at', 'a', 'the']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Functions to remove html strips and noise text and special characters (if they exist)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing the html strips\ndef strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply function on the description column\ntrain_df['description_lower']=train_df['description_lower'].apply(denoise_text)\ntest_df['description_lower']=test_df['description_lower'].apply(denoise_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define function for removing special characters\ndef remove_special_characters(text, remove_digits=True):\n    pattern=r'[^a-zA-z0-9\\s]'\n    text=re.sub(pattern,'',text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply function on review column\ntrain_df['description_lower']=train_df['description_lower'].apply(remove_special_characters)\ntest_df['description_lower']=test_df['description_lower'].apply(remove_special_characters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stemming the text\ndef simple_stemmer(text):\n    ps=nltk.porter.PorterStemmer()\n    text= ' '.join([ps.stem(word) for word in text.split()])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply function on review column\n#train_df['description_lower']=train_df['description_lower'].apply(simple_stemmer)\n#test_df['description_lower']=test_df['description_lower'].apply(simple_stemmer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove StopWords"},{"metadata":{"trusted":true},"cell_type":"code","source":"#set stopwords to english\n#stop=set(stopwords.words('english'))\nstop = set(stopword_list)\nprint(stop)\n\n#removing the stopwords\ndef remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply function on review column\n#train_df['description_lower']=train_df['description_lower'].apply(remove_stopwords)\n#test_df['description_lower']=test_df['description_lower'].apply(remove_stopwords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observe the sequence length distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we try to see the proportion of the length of the sentences\nlength_stats = [len(x.split()) for x in train_df['description_lower']]\n\nlength_stats_serie = pd.Series(length_stats)\nlength_stats_serie.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statistics as st\n\nstdev = st.stdev(length_stats)\nmean = st.mean(length_stats)\nquantile = np.quantile(length_stats, 0.9)\n\nprint(stdev)\nprint(mean)\nprint(quantile)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# can be up to 512 for BERT\nmax_length = 128","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding train and test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_sentences(sentences, bert_tokenizer):\n    input_ids=[]\n    attention_masks=[]\n\n    for sent in sentences:\n        bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_length,\n                                            pad_to_max_length = True,return_attention_mask = True)\n        input_ids.append(bert_inp['input_ids'])\n        attention_masks.append(bert_inp['attention_mask'])\n\n    input_ids=np.asarray(input_ids)\n    attention_masks=np.array(attention_masks)\n    \n    return [input_ids, attention_masks]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creation of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\nfrom transformers import TFBertModel, BertTokenizerFast, BertConfig\n\nmodel_name = 'bert-large-uncased'\n\n# Load transformers config and set output_hidden_states to False\nconfig = BertConfig.from_pretrained(model_name)\nconfig.output_hidden_states = False\n\ntokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config, do_lower_case=True)\n\n# Import the needed model(Bert, Roberta or DistilBert) with output_hidden_states=True\ntransformer_model = TFBertModel.from_pretrained(model_name, config = config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert = transformer_model.layers[0]\n\n# Build your model input\ninputs = Input(shape=(max_length,), name='input_ids', dtype='int32')\n\n# Load the Transformers BERT model as a layer in a Keras model\nbert_model = bert(inputs)[1]\ndropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\npooled_output = dropout(bert_model, training=False)\n\n\n# Then build your model output\njob = Dense(units=28, name='job', activation='softmax')(pooled_output)\n\n# And combine it all in a model object\nmodel = Model(inputs=inputs, outputs=job, name='BERT_job_prediction')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 3e-5\n\nnumber_of_epochs = 1\n\nbatch_size = 8\n\n# classifier Adam recommended\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08,\n                                    decay=0.01,\n                                    clipnorm=1.0)\n\n\nmodel.compile(optimizer=optimizer, \n              loss='categorical_crossentropy', \n              metrics=['accuracy', tf.keras.metrics.Precision(),\n                       tf.keras.metrics.Recall()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fine Tuning"},{"metadata":{},"cell_type":"markdown","source":"## Creating Checkpoints"},{"metadata":{"trusted":true},"cell_type":"code","source":"#for THE BASE BERT\n#filepath=OUTPUT_PATH+\"/model_bert768-{epoch:02d}-{val_accuracy:.3f}.hdf5\"\n\n#FOR THE LARGE BERT\nfilepath=OUTPUT_PATH+\"/bert-checkpoint.hdf5\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_callback = ModelCheckpoint(\n    filepath, monitor='val_accuracy', verbose=1,\n    save_best_only=False, save_weights_only=False,\n    save_frequency=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"#shuffle the train and test sets\nX_train, X_val, y_train, y_val = train_test_split(train_df['description_lower'], train_label['Category'], shuffle = True, test_size=0.10)\n\nX_train = convert_sentences(X_train, tokenizer)\nX_val = convert_sentences(X_val, tokenizer)\n\ny_train = to_categorical(y_train)\ny_val =  to_categorical(y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the data to the model\nhistory = model.fit(X_train, y_train,\n                    validation_data=(X_val, y_val),\n                    epochs=number_of_epochs,\n                    batch_size=batch_size,\n                    verbose = 1,\n                    validation_freq=1,\n                    callbacks=[checkpoint_callback]\n                   )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the trained model\n\n#for the base bert\nmodel.save(OUTPUT_PATH+'/nlp_model_job_prediction_bert_classification.h5') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict on test dataset\nmodel.evaluate(X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyze the performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the pretrained nlp_model\nfrom tensorflow.keras.models import load_model\nnew_model = load_model(OUTPUT_PATH+'/nlp_model_job_prediction_bert_classification.h5',custom_objects={'KerasLayer':hub.KerasLayer})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test2 = convert_sentences(test_df['description_lower'], tokenizer)\npredictions = np.argmax(new_model.predict(X_test2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## File Generation"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[\"Category\"] = predictions\nbaseline_file = test_df[[\"Id\",\"Category\"]]\nbaseline_file.to_csv(\"/kaggle/working/baseline.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}