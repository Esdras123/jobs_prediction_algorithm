{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler,ReduceLROnPlateau\nimport seaborn as sns\nimport transformers\n\nimport nltk\nimport re\n\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n\nplt.style.use('seaborn')\npd.set_option('display.max_colwidth',1000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print(tf.__version__)\nprint(tf.config.list_physical_devices('GPU'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data import"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/defi-ia-insa-toulouse\"\nOUTPUT_PATH = \"/kaggle/working\"\ntrain_df = pd.read_json(DATA_PATH+\"/train.json\")\ntest_df = pd.read_json(DATA_PATH+\"/test.json\")\ntrain_label = pd.read_csv(DATA_PATH+\"/train_label.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Observation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify missing values\ntrain_df.apply(lambda x: sum(x.isnull()), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the target class balance\ntrain_label[\"Category\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.description.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"## Cleaning Process"},{"metadata":{"trusted":true},"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"description_modified\"] = [x.lower() for x in train_df.description]\ntest_df[\"description_modified\"] = [x.lower() for x in test_df.description]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing the html strips if it exists\ndef strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply function on the description column\ntrain_df['description_modified']=train_df['description_modified'].apply(denoise_text)\ntest_df['description_modified']=test_df['description_modified'].apply(denoise_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define function for removing special characters\ndef remove_special_characters(text, remove_digits=True):\n    pattern=r'[^a-zA-z0-9\\s]'\n    text=re.sub(pattern,'',text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply function on review column\ntrain_df['description_modified']=train_df['description_modified'].apply(remove_special_characters)\ntest_df['description_modified']=test_df['description_modified'].apply(remove_special_characters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply function on review column\n\n#train_df['description_modified']=train_df['description_modified'].apply(simple_stemmer)\n#test_df['description_modified']=test_df['description_modified'].apply(simple_stemmer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.tokenize.toktok import ToktokTokenizer\n\n#Tokenization of text\ntokenizer=ToktokTokenizer()\n#Setting English stopwords\nstopword_list= stopwords.words('english')\n\n#stopword_list = ['in', 'of', 'at', 'a', 'the']\n\n#set stopwords to english\n#stop=set(stopwords.words('english'))\nstop = set(stopword_list)\nprint(stop)\n\n#removing the stopwords\ndef remove_stopwords(text, is_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply function on review column\n\ntrain_df['description_modified']=train_df['description_modified'].apply(remove_stopwords)\ntest_df['description_modified']=test_df['description_modified'].apply(remove_stopwords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observe the sequence length distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we try to see the proportion of the length of the sentences\nlength_stats = [len(x.split()) for x in train_df['description_modified']]\n\nlength_stats_serie = pd.Series(length_stats)\nlength_stats_serie.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Observe the long datas to see if the important parts of the sentences are the first or last sentences\n#for sen in train_df[\"description\"]:\n#    if len(sen)>120:\n#        print(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statistics as st\n\nstdev = st.stdev(length_stats)\nmean = st.mean(length_stats)\nquantile = np.quantile(length_stats, 0.7)\n\nprint(stdev)\nprint(mean)\nprint(quantile)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Take the best sequence length for the next part"},{"metadata":{"trusted":true},"cell_type":"code","source":" # CHOSEN sequence length\n#CHOSEN_SEQ_LEN = int(quantile)\n\nCHOSEN_SEQ_LEN = 45","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creation Of The XLNet Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import TFXLNetModel, XLNetTokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is the identifier of the model. The library need this ID to download the weights and initialize the architecture\n# here is all the supported ones:\n# https://huggingface.co/transformers/pretrained_models.html\nxlnet_model = 'xlnet-large-cased'\nxlnet_tokenizer = XLNetTokenizer.from_pretrained(xlnet_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_xlnet(mname, shape = 120):\n    \"\"\" Creates the model. It is composed of the XLNet main block and then\n    a classification head its added\n    \"\"\"\n    # Define token ids as inputs\n    word_inputs = Input(shape=(shape,), name='word_inputs', dtype='int32')\n\n    # Call XLNet model\n    xlnet = TFXLNetModel.from_pretrained(mname)\n    x = xlnet(word_inputs)[0]\n\n    ##########\n    #change the shape of x to remove the sequence length\n    x = tf.squeeze(x[:, -1:, :], axis=1)\n    \n    # Add a hidden layer\n    x = Dense(units=1024, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    \n    # Add a hidden layer\n    x = Dense(units=1024, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    \n    # Final output \n    outputs = Dense(28, activation='softmax', name='outputs')(x)\n\n    # Compile model\n    model = Model(inputs=[word_inputs], outputs=[outputs])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xlnet = create_xlnet(xlnet_model, CHOSEN_SEQ_LEN)\nxlnet.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Work on Data"},{"metadata":{},"cell_type":"markdown","source":"## Split Train Val"},{"metadata":{"trusted":true},"cell_type":"code","source":"#shuffle the train and test sets\nX_train, X_val, y_train, y_val = train_test_split(train_df['description_modified'], train_label['Category'], shuffle = True, test_size=0.2)\n\ny_train = to_categorical(y_train)\ny_val =  to_categorical(y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transform the input"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_inputs(data, tokenizer, max_len=120):\n    \"\"\" Gets tensors from text using the tokenizer provided\"\"\"\n    inps = [tokenizer.encode_plus(t, max_length=max_len, pad_to_max_length=True, add_special_tokens=True) for t in data]\n    inp_tok = np.array([a['input_ids'] for a in inps])\n    ids = np.array([a['attention_mask'] for a in inps])\n    segments = np.array([a['token_type_ids'] for a in inps])\n    return inp_tok, ids, segments\n\ndef warmup(epoch, lr):\n    \"\"\"Used for increasing the learning rate slowly, this tends to achieve better convergence.\n    However, as we are finetuning for few epoch it's not crucial.\n    \"\"\"\n    return max(lr +1e-6, 2e-5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compile the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"xlnet.compile(optimizer= Adam(lr=2e-5), loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"inp_tok, ids, segments = get_inputs(X_train, xlnet_tokenizer, CHOSEN_SEQ_LEN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create Checkpoints"},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath=OUTPUT_PATH+\"/model_xlnet-{epoch:02d}-{val_accuracy:.3f}.hdf5\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_callback = ModelCheckpoint(\n    filepath, monitor='val_accuracy', verbose=1,\n    save_best_only=False, save_weights_only=False,\n    save_frequency=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create the different callbacks"},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping_callback = EarlyStopping(monitor='val_accuracy', \n                  patience=4, min_delta=0.02, \n                  restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_scheduler = LearningRateScheduler(warmup, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reducelr_callback = ReduceLROnPlateau(monitor='val_accuracy',\n                      factor=1e-6, patience=2, verbose=0, \n                      mode='auto', min_delta=0.001, cooldown=0, min_lr=1e-6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [\n    early_stopping_callback\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = xlnet.fit(x=inp_tok, y=y_train, \n                 epochs=2, \n                 batch_size=32,\n                 callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"inp_tok, ids, segments = get_inputs(X_val, xlnet_tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.argmax(xlnet.predict(inp_tok, verbose=True), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyse the performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict on test dataset\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(np.argmax(y_val,axis=1), preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission File Generation"},{"metadata":{"trusted":true},"cell_type":"code","source":"inp_tok, ids, segments = get_inputs(test_df['description_modified'], xlnet_tokenizer)\n\npredictions = np.argmax(xlnet.predict(inp_tok), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[\"Category\"] = predictions\nresults_file = test_df[[\"Id\",\"Category\"]]\nresults_file.to_csv(OUTPUT_PATH+\"/results_xlnet.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}